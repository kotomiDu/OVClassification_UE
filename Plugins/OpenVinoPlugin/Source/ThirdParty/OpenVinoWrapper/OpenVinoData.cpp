/*
"Copyright 2019 Intel Corporation.

The source code, information and material ("Material") contained herein is owned by Intel Corporation or its
suppliers or licensors, and title to such Material remains with Intel Corporation or its suppliers or licensors.
The Material contains proprietary information of Intel or its suppliers and licensors. The Material is
protected by worldwide copyright laws and treaty provisions. No part of the Material may be used, copied,
reproduced, modified, published, uploaded, posted, transmitted, distributed or disclosed in any way without Intel's
prior express written permission. No license under any patent, copyright or other intellectual property rights in
the Material is granted to or conferred upon you, either expressly, by implication, inducement, estoppel or otherwise.
Any license under such intellectual property rights must be express and approved by Intel in writing.


Include supplier trademarks or logos as supplier requires Intel to use, preceded by an asterisk. An asterisked footnote
can be added as follows: *Third Party trademarks are the property of their respective owners.

Unless otherwise agreed by Intel in writing, you may not remove or alter this notice or any other notice
embedded in Materials by Intel or Intel's suppliers or licensors in any way."
*/



// OpenVinoTestDll.cpp : Defines the entry point for the application.
//

#include "OpenVinoData.h"

#include <iomanip>
#include <fstream>
#include <vector>
#include <memory>
#include <cstdlib>
#include <string>
#include <limits>

#include <opencv2/opencv.hpp>
#include <inference_engine.hpp>

#include "ClassificationResult.h"

using namespace std;
using namespace InferenceEngine;

/*
 * @brief Initialize OpenVino with passed model files
 * @param modelXmlFilePath
 * @param modelBinFilePath
 * @param modelLabelFilePath
 */
void 
OpenVinoData::Initialize(
	string modelXmlFilePath,
	string modelBinFilePath,
	string modelLabelFilePath)
{

	// --------------------------- 2. Read IR Generated by ModelOptimizer (.xml and .bin files) ------------
	clog << "2. Read IR..." << endl;
	Core core;
	/** Set batch size to 1 **/
	CNNNetwork network = core.ReadNetwork(modelXmlFilePath);
	auto shapes = network.getInputShapes();
	for (auto& shape : shapes)
		shape.second[0] = 1;
	network.reshape(shapes);

	// --------------------------- 3. Configure input & output ---------------------------------------------
	clog << "3. Configure input/output..." << endl;
	input_info = network.getInputsInfo().begin()->second;
	input_name = network.getInputsInfo().begin()->first;

	input_info->setLayout(Layout::NCHW);
	input_info->setPrecision(Precision::U8);

	DataPtr output_info = network.getOutputsInfo().begin()->second;
	output_name = network.getOutputsInfo().begin()->first;

	output_info->setPrecision(Precision::FP32);

	// --------------------------- 4. Loading model to the plugin ------------------------------------------
	clog << "4. Loading model..." << endl;
	clog << "4. Loading model..." << endl;
	executable_network = core.LoadNetwork(network, "CPU");//, cnnConfig.execNetworkConfig);

	labels = LoadLabels(modelLabelFilePath);

	clog << "Intialized." << endl;
}

/*
 * @brief Call infer using loaded model files
 * @param filePath
 * @param modelBinFilePath
 * @param modelLabelFilePath
 */
string 
OpenVinoData::Infer(
	string file_path)
{

	// --------------------------- 5. Create infer request -------------------------------------------------
	clog << "5. Creating request..." << endl;
	InferRequest infer_request = executable_network.CreateInferRequest();

	// return image_file_name;

	// --------------------------- 6. Prepare input --------------------------------------------------------
	cout << "6. Prepare input..." << endl;
	cv::Mat image = cv::imread(file_path);

	/* Resize manually and copy data from the image to the input blob */
	Blob::Ptr input = infer_request.GetBlob(input_name);
	auto input_data = input->buffer().as<PrecisionTrait<Precision::U8>::value_type*>();

	auto size = cv::Size(input_info->getTensorDesc().getDims()[3], input_info->getTensorDesc().getDims()[2]);
	cv::resize(image, image, size);
	cv::imwrite("test.png", image);
	size_t channels_number = input->getTensorDesc().getDims()[1];
	size_t image_size = input->getTensorDesc().getDims()[3] * input->getTensorDesc().getDims()[2];

	for (size_t pid = 0; pid < image_size; ++pid) {
		for (size_t ch = 0; ch < channels_number; ++ch) {
			input_data[ch * image_size + pid] = image.at<cv::Vec3b>(pid)[ch];
		}
	}
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 7. Do inference --------------------------------------------------------
	clog << "7. Do inference..." << endl;
	/* Running the request synchronously */
	infer_request.Infer();
	// -----------------------------------------------------------------------------------------------------

	// --------------------------- 8. Process output ------------------------------------------------------
	clog << "8. Process output..." << endl;
	Blob::Ptr output = infer_request.GetBlob(output_name);

	// Print classification results
	ClassificationResult classificationResult(output, { file_path.c_str() }, 1, 10, labels);
	auto result = classificationResult.toString();

	return result;
}

/*static method*/
vector<string>
OpenVinoData::LoadLabels(
	string labelsFilePath)
{
	vector<string> labels;

	ifstream labels_file(labelsFilePath, ios::in);
	if (labels_file.is_open())
	{
		string label;
		while (getline(labels_file, label))
		{
			trim(label);
			labels.push_back(label);
		}
	}

	return labels;
}
